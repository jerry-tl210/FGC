{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence \n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import AdamW\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_json(\"FGC_release_1.7.13/FGC_release_all_dev.json\")\n",
    "training_data = pd.read_json(\"FGC_release_1.7.13/FGC_release_all_train.json\")\n",
    "test_data = pd.read_json(\"FGC_release_1.7.13/FGC_release_all_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocessing(data, is_dev=False):\n",
    "    \n",
    "    # Save all the questions, potential supporting evidence and indices in three lists\n",
    "    textQ_to_be_tokenized = []\n",
    "    textA_to_be_tokenized = []\n",
    "    sp_index = []\n",
    "    \n",
    "    for dictionary in data['QUESTIONS']:\n",
    "        for element in dictionary:\n",
    "            textQ_to_be_tokenized.append(element['QTEXT_CN'])\n",
    "            sp_index.append(element['SHINT_'])\n",
    "    for dictionary in data['SENTS']:\n",
    "        current_text_sentence = []\n",
    "        for element in dictionary:\n",
    "            current_text_sentence.append(element['text'])\n",
    "        textA_to_be_tokenized.append(current_text_sentence)\n",
    "    \n",
    "    QandA_label = pd.DataFrame({'Question': textQ_to_be_tokenized,\n",
    "                                'Sentence_List': textA_to_be_tokenized,\n",
    "                                'SE_Index': sp_index,\n",
    "                                'Label': sp_index})\n",
    "    \n",
    "    QandA_label['Length'] = QandA_label['Sentence_List'].apply(lambda x: len(x))\n",
    "    QandA_label['SE_Index'] = QandA_label['SE_Index'].apply(lambda x: [0])\n",
    "    QandA_label['SE_Index'] = QandA_label['SE_Index'] * QandA_label['Length']\n",
    "    QandA_label['SE_Index'] = list(zip(QandA_label['SE_Index'], QandA_label['Label']))\n",
    "\n",
    "    # Extract label index\n",
    "    for row in QandA_label['SE_Index']:\n",
    "        for index in row[1]:\n",
    "            row[0][index] = 1\n",
    "        \n",
    "    indexed = [i[0] for i in list(QandA_label['SE_Index'])]\n",
    "    QandA_label['Label'] = indexed\n",
    "\n",
    "    Q_and_Sentence_all_Comb = pd.DataFrame({'Question':np.repeat(QandA_label['Question'].values, QandA_label['Sentence_List'].str.len()),\n",
    "                        'Sentence':np.concatenate(QandA_label['Sentence_List'].values)})\n",
    "    Q_and_Sentence_all_Comb['Label'] = QandA_label['Label'].sum()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "            \n",
    "    # Put all question and sentence combination into a list \n",
    "    All_instances = []\n",
    "    for i in range(len(QandA_label)):\n",
    "        for sentence in QandA_label['Sentence_List'][i]:\n",
    "            question_token = tokenizer.tokenize(QandA_label['Question'][i])\n",
    "            sentence_token = tokenizer.tokenize(sentence)\n",
    "            instance = ['[CLS]'] + question_token + ['[SEP]'] + sentence_token + ['[SEP]']\n",
    "            if len(instance) > 512:\n",
    "                instance = instance[:512]\n",
    "            All_instances.append(instance)\n",
    "            \n",
    "    # Convert ids to segment_ids\n",
    "    segment_ids = []\n",
    "    for token in All_instances:\n",
    "        length_of_zeros = token.index('[SEP]') - token.index('[CLS]') + 1\n",
    "        length_of_ones = len(token) - length_of_zeros\n",
    "        zeros_and_ones = [0] * length_of_zeros + [1] * length_of_ones\n",
    "        segment_ids.append(zeros_and_ones)\n",
    "        \n",
    "    ids = []\n",
    "    for token in All_instances:\n",
    "        ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "        \n",
    "    mask_ids = []\n",
    "    for token in All_instances:\n",
    "        mask_ids.append([1] * len(token))\n",
    "        \n",
    "    labels = list(Q_and_Sentence_all_Comb['Label'])\n",
    "    labels = [[i] for i in labels]\n",
    "    \n",
    "    return All_instances, ids, segment_ids, mask_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0703 01:59:50.011849 140491409098560 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "I0703 01:59:54.834610 140491409098560 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "dev_instances, dev_ids, dev_seg_ids, dev_mask_ids, dev_labels = datapreprocessing(validation_data, True)\n",
    "train_instances, train_ids, train_seg_ids, train_mask_ids, train_labels = datapreprocessing(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ids, segment_ids, mask_ids, labels):\n",
    "        self.instances = []\n",
    "        for ids_i, segment_ids_i, mask_ids, label in zip(ids, segment_ids, mask_ids, labels):\n",
    "            self.instances.append({\"ids\": ids_i, \"segment_ids\": segment_ids_i, \n",
    "                                   \"mask_ids\": mask_ids, \"labels\": label})  \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.instances[idx]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentenceDataset(train_ids, train_seg_ids, train_mask_ids, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = SentenceDataset(dev_ids, dev_seg_ids, dev_mask_ids, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    padded_ids = pad_sequence([torch.tensor(instance['ids']) for instance in batch], batch_first=True)\n",
    "    padded_ids = padded_ids.to(device)\n",
    "    \n",
    "    padded_segment_ids = pad_sequence([torch.tensor(instance['segment_ids']) for instance in batch], batch_first=True)\n",
    "    padded_segment_ids = padded_segment_ids.to(device)\n",
    "    \n",
    "    padded_mask_ids = pad_sequence([torch.tensor(instance['mask_ids']) for instance in batch], batch_first=True)\n",
    "    padded_mask_ids = padded_mask_ids.to(device)\n",
    "    \n",
    "    labels = torch.stack([torch.tensor(instance['labels']) for instance in batch])\n",
    "    labels = labels.to(device)\n",
    "    return {'ids': padded_ids, 'mask_ids': padded_mask_ids, 'segment_ids': padded_segment_ids, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(train_dataset, batch_size=8, shuffle = True, collate_fn = collate)\n",
    "#dataloader_dev = DataLoader(dev_dataset, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocessing(data, dataset):\n",
    "    \n",
    "    len_array = np.cumsum(np.array(data['SENTS'].apply(lambda x: len(x))))\n",
    "\n",
    "    dictionary_lists = []\n",
    "    batches = []\n",
    "    for i in range(len(dataset.instances)):\n",
    "\n",
    "        instance = dataset.instances[i]\n",
    "        dictionary_lists.append(instance)\n",
    "\n",
    "        if i in len_array - 1:\n",
    "\n",
    "            padded_ids = pad_sequence([torch.tensor(instance['ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_ids = padded_ids.to(device)\n",
    "\n",
    "            padded_segment_ids = pad_sequence([torch.tensor(instance['segment_ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_segment_ids = padded_segment_ids.to(device)\n",
    "\n",
    "            padded_mask_ids = pad_sequence([torch.tensor(instance['mask_ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_mask_ids = padded_mask_ids.to(device)\n",
    "\n",
    "            labels = torch.stack([torch.tensor(instance['labels']) for instance in dictionary_lists])\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            current_dev_batch = {'ids': padded_ids, 'mask_ids': padded_mask_ids, 'segment_ids': padded_segment_ids, 'labels': labels}\n",
    "\n",
    "            batches.append(current_dev_batch)\n",
    "            dictionary_lists = []\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGC_Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(FGC_Network, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # batch['ids'] = (batch_size, sent_len)\n",
    "        # batch['segment_ids'] = (batch_size, sent_len)\n",
    "        # batch['mask_ids'] = = (batch_size, sent_len)\n",
    "        # output = (batch_size, 1)\n",
    "        hidden_state, pooler_output = self.bert(batch['ids'], batch['mask_ids'], batch['segment_ids'])\n",
    "        linear_output = self.linear(pooler_output)\n",
    "        \n",
    "        return linear_output\n",
    "\n",
    "    def loss(self, batch):\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        output = self.forward(batch)\n",
    "        target = batch['labels'].float().to(device)\n",
    "        \n",
    "        return loss_fn(output, target)\n",
    "    \n",
    "    def _predict(self, batch):\n",
    "        \n",
    "        output = self.forward(batch)\n",
    "        scores = torch.sigmoid(output)\n",
    "        scores = scores.cpu().numpy()[:,0].tolist()\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict_fgc(self, batch, threshold=0.5):\n",
    "        \n",
    "        scores = self._predict(batch)\n",
    "\n",
    "        max_i = 0\n",
    "        max_score = 0\n",
    "        sp = []\n",
    "        \n",
    "        for i, score in enumerate(scores):\n",
    "\n",
    "            if score > max_score:\n",
    "                max_i = i\n",
    "                max_score = score\n",
    "            if score >= threshold:\n",
    "                sp.append(i)\n",
    "\n",
    "        if not sp:\n",
    "            sp.append(max_i)\n",
    "\n",
    "        return {'sp': sp, 'sp_scores': scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0703 02:00:14.653911 140491409098560 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "I0703 02:00:14.657922 140491409098560 configuration_utils.py:169] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0703 02:00:15.517849 140491409098560 modeling_utils.py:387] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FGC_Network(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = FGC_Network()\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim(nn, num_epochs, lr):\n",
    "    param_optimizer = list(nn.bert.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_epochs = num_epochs\n",
    "    num_train_optimization_steps = len(dataloader_train) * num_epochs\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                     num_warmup_steps=int(\n",
    "                                                         num_train_optimization_steps * 0.1),\n",
    "                                                     num_training_steps=num_train_optimization_steps)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_sp(metrics, sp_gold, sp_pred):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "        \n",
    "    for p in sp_pred:\n",
    "        if p in sp_gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for g in sp_gold:\n",
    "        if g not in sp_pred:\n",
    "            fn += 1\n",
    "            \n",
    "    precision = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "    \n",
    "    metrics['sp_em'] += em\n",
    "    metrics['sp_f1'] += f1\n",
    "    metrics['sp_prec'] += precision\n",
    "    metrics['sp_recall'] += recall\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sp_fgc(sp_golds, sp_preds):\n",
    "    \n",
    "    metrics = {'sp_em': 0, 'sp_prec': 0, 'sp_recall': 0, 'sp_f1': 0}\n",
    "    \n",
    "    assert len(sp_golds) == len(sp_preds)\n",
    "    \n",
    "    for sp_gold, sp_pred in zip(sp_golds, sp_preds):\n",
    "        _update_sp(metrics, sp_gold, sp_pred)\n",
    "    \n",
    "    N = len(sp_golds)\n",
    "    for k in metrics.keys():\n",
    "        metrics[k] /= N\n",
    "        metrics[k] = round(metrics[k], 3)\n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fgc_atype(atype_golds, atype_preds):\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    for gold, atype in zip(atype_golds, atype_preds):\n",
    "        if atype == gold:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    return pos/len(atypes_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(network, dev_batches, current_epoch, sp_golds, avg_loss):\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sp_preds = []\n",
    "        \n",
    "        #atype_preds = []\n",
    "        for batch in tqdm(dev_batches):\n",
    "            \n",
    "            out_dct = network.predict_fgc(batch)\n",
    "            sp_preds.append(out_dct['sp'])\n",
    "            \n",
    "            #if 'atype' in out_dct:\n",
    "                #for type_i in out_dct['atype']:\n",
    "                    #assert type_i == out_dct['atype'][0]\n",
    "                #atype_preds.append(type_i)\n",
    "                \n",
    "  \n",
    "    metrics = eval_sp_fgc(sp_golds, sp_preds)\n",
    "    print('epoch %d eval_recall: %.3f eval_f1: %.3f loss: %.3f' % (\n",
    "            current_epoch, metrics['sp_recall'], metrics['sp_f1'], avg_loss))\n",
    "        \n",
    "    #torch.save(network.state_dict(), \"FGC_release_1.7.13/models_with_scheduler/model_epoch{0}_eval_em:{1:.3f}_precision:{2:.3f}_recall:{3:.3f}_f1:{4:.3f}_loss:{5:.3f}.m\".format(current_epoch, metrics['sp_em'], metrics['sp_prec'], metrics['sp_recall'], metrics['sp_f1'], avg_loss))\n",
    "    \n",
    "    return sp_preds, sp_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, num_epochs, lr):\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer, scheduler = optim(network, num_epochs, lr)\n",
    "    \n",
    "    sp_golds = validation_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist()\n",
    "    #atype_golds = validation_data['QUESTIONS'].apply(lambda x: x[0]['ATYPE_']).tolist()\n",
    "    \n",
    "    for current_epoch in range(num_epochs):\n",
    "        network.train()\n",
    "        running_loss = 0.0\n",
    "        dr = True\n",
    "        for batch in tqdm(dataloader_train):\n",
    "            optimizer.zero_grad()\n",
    "            current_output = network(batch)\n",
    "            current_target = batch['labels'].to(dtype=torch.float, device=device)\n",
    "            current_loss = loss_fn(current_output, current_target)\n",
    "\n",
    "            current_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(network.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += current_loss.item()\n",
    "            \n",
    "        learning_rate_scalar = scheduler.get_lr()[0]\n",
    "        print('lr = %f' % learning_rate_scalar)\n",
    "        avg_loss = running_loss/len(dataloader_train)\n",
    "        print('epoch %d train_loss: %.3f' % (current_epoch, avg_loss))\n",
    "        eval(network, dev_batches, current_epoch, sp_golds, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2a53805bd946c5bf3541b295e8e353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000010\n",
      "epoch 0 train_loss: 0.213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e93fe6a68b4444f93b9726b421391bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.142, 'sp_prec': 0.592, 'sp_recall': 0.611, 'sp_f1': 0.543}\n",
      "epoch 0 eval_recall: 0.611 eval_f1: 0.543 loss: 0.213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5042101ea81246228c5ee50c3c0f7324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000020\n",
      "epoch 1 train_loss: 0.163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe0ea2c6ebb4569a4b674374c37b39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.15, 'sp_prec': 0.59, 'sp_recall': 0.63, 'sp_f1': 0.554}\n",
      "epoch 1 eval_recall: 0.630 eval_f1: 0.554 loss: 0.163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b15232cf7b4ea88fb5b77c9d80d2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000019\n",
      "epoch 2 train_loss: 0.135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102366f09ad14a8caf0499b374d4bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.166, 'sp_prec': 0.619, 'sp_recall': 0.532, 'sp_f1': 0.522}\n",
      "epoch 2 eval_recall: 0.532 eval_f1: 0.522 loss: 0.135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bfab1ec5984ff28730bb0d0b578e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000018\n",
      "epoch 3 train_loss: 0.106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb88ec8b762b43d8b7fb9dd576b1675e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.186, 'sp_prec': 0.661, 'sp_recall': 0.532, 'sp_f1': 0.548}\n",
      "epoch 3 eval_recall: 0.532 eval_f1: 0.548 loss: 0.106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc3b4cb31dd4888b626ba71c51c6f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000017\n",
      "epoch 4 train_loss: 0.083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9a8ba3874442f28038024a6d96a20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.174, 'sp_prec': 0.627, 'sp_recall': 0.545, 'sp_f1': 0.538}\n",
      "epoch 4 eval_recall: 0.545 eval_f1: 0.538 loss: 0.083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4e810f2092490d9857210c5c1f52f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000016\n",
      "epoch 5 train_loss: 0.062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d82e8927484b38a7341e007d385782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.182, 'sp_prec': 0.59, 'sp_recall': 0.614, 'sp_f1': 0.547}\n",
      "epoch 5 eval_recall: 0.614 eval_f1: 0.547 loss: 0.062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3598dfce18474337918545bc2065b54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000014\n",
      "epoch 6 train_loss: 0.050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3ed96801bd4030a5ffd2688a00890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.154, 'sp_prec': 0.565, 'sp_recall': 0.617, 'sp_f1': 0.538}\n",
      "epoch 6 eval_recall: 0.617 eval_f1: 0.538 loss: 0.050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9027e33631914473a26087dd486cc8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000013\n",
      "epoch 7 train_loss: 0.039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285429c3c8364bf295bd8d493d6fc794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.146, 'sp_prec': 0.535, 'sp_recall': 0.627, 'sp_f1': 0.517}\n",
      "epoch 7 eval_recall: 0.627 eval_f1: 0.517 loss: 0.039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1082bf78dd49d59baa3545334913ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(network, 20, 0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0703 05:30:48.277780 140491409098560 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "I0703 05:30:48.282982 140491409098560 configuration_utils.py:169] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0703 05:30:49.519857 140491409098560 modeling_utils.py:387] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_network = FGC_Network()\n",
    "trained_network.load_state_dict(torch.load('FGC_release_1.7.13/models_with_scheduler/model_epoch19_eval_em:0.154_precision:0.599_recall:0.609_f1:0.547_loss:0.001.m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99e10c0f836465d9bd45acf004ff8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=882), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.985, 'sp_prec': 0.986, 'sp_recall': 0.988, 'sp_f1': 0.987}\n",
      "epoch 0 eval_recall: 0.988 eval_f1: 0.987 loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "sp_golds = training_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist()\n",
    "batches = eval_preprocessing(training_data, train_dataset)\n",
    "\n",
    "trained_network.to(\"cuda\")\n",
    "train_pred, train_obs = eval(trained_network, batches, 0, sp_golds, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_with_performance = training_data.copy()\n",
    "training_data_with_performance['train_pred'] = train_pred\n",
    "training_data_with_performance['train_obs'] = train_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DID</th>\n",
       "      <th>QUESTIONS</th>\n",
       "      <th>DTEXT</th>\n",
       "      <th>DTEXT_CN</th>\n",
       "      <th>SENTS</th>\n",
       "      <th>train_pred</th>\n",
       "      <th>train_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>D006</td>\n",
       "      <td>[{'QID': 'D006Q02', 'QTYPE': '', 'ATYPE_': '...</td>\n",
       "      <td> ...</td>\n",
       "      <td> ...</td>\n",
       "      <td>[{'text': ' ...</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>D032</td>\n",
       "      <td>[{'QID': 'D032Q10', 'QTYPE': '', 'ATYPE_': ...</td>\n",
       "      <td>North American Free Trade Agreemen...</td>\n",
       "      <td>North American Free Trade Agreemen...</td>\n",
       "      <td>[{'text': 'North American Free Tra...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>D048</td>\n",
       "      <td>[{'QID': 'D048Q09', 'QTYPE': '', 'ATYPE_': '...</td>\n",
       "      <td>MIT1966...</td>\n",
       "      <td>MIT1966...</td>\n",
       "      <td>[{'text': '', 'start': 0, 'e...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>D086</td>\n",
       "      <td>[{'QID': 'D086Q03', 'QTYPE': '', 'ATYPE_': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': '', 'start': 0, 'end': 6, 'IE'...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>D094</td>\n",
       "      <td>[{'QID': 'D094Q01', 'QTYPE': '', 'ATYPE_': '...</td>\n",
       "      <td>Eden Social Welfare Foundatio...</td>\n",
       "      <td>Eden Social Welfare Foundatio...</td>\n",
       "      <td>[{'text': 'Eden Social Welfar...</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>D107</td>\n",
       "      <td>[{'QID': 'D107Q07', 'QTYPE': '', 'ATYPE_': ...</td>\n",
       "      <td>\\n...</td>\n",
       "      <td>\\n...</td>\n",
       "      <td>[{'text': '', 'start': 0, 'end': ...</td>\n",
       "      <td>[12, 25]</td>\n",
       "      <td>[25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>D107</td>\n",
       "      <td>[{'QID': 'D107Q08', 'QTYPE': '', 'ATYPE_': ...</td>\n",
       "      <td>\\n...</td>\n",
       "      <td>\\n...</td>\n",
       "      <td>[{'text': '', 'start': 0, 'end': ...</td>\n",
       "      <td>[12, 25]</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>D116</td>\n",
       "      <td>[{'QID': 'D116Q09', 'QTYPE': '', 'ATYPE_': ...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': '', 'st...</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>D182</td>\n",
       "      <td>[{'QID': 'D182Q07', 'QTYPE': '', 'ATYPE_': ...</td>\n",
       "      <td>924324...</td>\n",
       "      <td>924324...</td>\n",
       "      <td>[{'text': '9243', 'start': 0, 'e...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>D245</td>\n",
       "      <td>[{'QID': 'D245Q05', 'QTYPE': '', 'ATYPE_': '...</td>\n",
       "      <td>15...</td>\n",
       "      <td>15...</td>\n",
       "      <td>[{'text': '15', 'start': 0, 'end': 7, 'IE...</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>D272</td>\n",
       "      <td>[{'QID': 'D272Q09', 'QTYPE': '', 'ATYPE_': ...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': '', 'start':...</td>\n",
       "      <td>[62]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>D288</td>\n",
       "      <td>[{'QID': 'D288Q12', 'QTYPE': '', 'ATYPE_': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'text': '', 'start': 0, 'end': 9, '...</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>D322</td>\n",
       "      <td>[{'QID': 'D322Q07', 'QTYPE': '', 'ATYPE_': '...</td>\n",
       "      <td>171...</td>\n",
       "      <td>171...</td>\n",
       "      <td>[{'text': '', 'start': 0, 'end':...</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      DID                                          QUESTIONS  \\\n",
       "20   D006  [{'QID': 'D006Q02', 'QTYPE': '', 'ATYPE_': '...   \n",
       "70   D032  [{'QID': 'D032Q10', 'QTYPE': '', 'ATYPE_': ...   \n",
       "156  D048  [{'QID': 'D048Q09', 'QTYPE': '', 'ATYPE_': '...   \n",
       "284  D086  [{'QID': 'D086Q03', 'QTYPE': '', 'ATYPE_': '...   \n",
       "324  D094  [{'QID': 'D094Q01', 'QTYPE': '', 'ATYPE_': '...   \n",
       "370  D107  [{'QID': 'D107Q07', 'QTYPE': '', 'ATYPE_': ...   \n",
       "371  D107  [{'QID': 'D107Q08', 'QTYPE': '', 'ATYPE_': ...   \n",
       "395  D116  [{'QID': 'D116Q09', 'QTYPE': '', 'ATYPE_': ...   \n",
       "449  D182  [{'QID': 'D182Q07', 'QTYPE': '', 'ATYPE_': ...   \n",
       "502  D245  [{'QID': 'D245Q05', 'QTYPE': '', 'ATYPE_': '...   \n",
       "630  D272  [{'QID': 'D272Q09', 'QTYPE': '', 'ATYPE_': ...   \n",
       "731  D288  [{'QID': 'D288Q12', 'QTYPE': '', 'ATYPE_': '...   \n",
       "874  D322  [{'QID': 'D322Q07', 'QTYPE': '', 'ATYPE_': '...   \n",
       "\n",
       "                                                 DTEXT  \\\n",
       "20    ...   \n",
       "70   North American Free Trade Agreemen...   \n",
       "156  MIT1966...   \n",
       "284  ...   \n",
       "324  Eden Social Welfare Foundatio...   \n",
       "370  \\n...   \n",
       "371  \\n...   \n",
       "395  ...   \n",
       "449  924324...   \n",
       "502  15...   \n",
       "630  ...   \n",
       "731  ...   \n",
       "874  171...   \n",
       "\n",
       "                                              DTEXT_CN  \\\n",
       "20    ...   \n",
       "70   North American Free Trade Agreemen...   \n",
       "156  MIT1966...   \n",
       "284  ...   \n",
       "324  Eden Social Welfare Foundatio...   \n",
       "370  \\n...   \n",
       "371  \\n...   \n",
       "395  ...   \n",
       "449  924324...   \n",
       "502  15...   \n",
       "630  ...   \n",
       "731  ...   \n",
       "874  171...   \n",
       "\n",
       "                                                 SENTS train_pred train_obs  \n",
       "20   [{'text': ' ...        [9]        []  \n",
       "70   [{'text': 'North American Free Tra...       [11]        []  \n",
       "156  [{'text': '', 'start': 0, 'e...        [1]        []  \n",
       "284  [{'text': '', 'start': 0, 'end': 6, 'IE'...        [3]        []  \n",
       "324  [{'text': 'Eden Social Welfar...        [4]        []  \n",
       "370  [{'text': '', 'start': 0, 'end': ...   [12, 25]      [25]  \n",
       "371  [{'text': '', 'start': 0, 'end': ...   [12, 25]      [12]  \n",
       "395  [{'text': '', 'st...       [10]        []  \n",
       "449  [{'text': '9243', 'start': 0, 'e...        [2]        []  \n",
       "502  [{'text': '15', 'start': 0, 'end': 7, 'IE...        [7]        []  \n",
       "630  [{'text': '', 'start':...       [62]        []  \n",
       "731  [{'text': '', 'start': 0, 'end': 9, '...        [9]        []  \n",
       "874  [{'text': '', 'start': 0, 'end':...       [22]        []  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_with_performance[training_data_with_performance['train_pred'] != training_data_with_performance['train_obs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'QID': 'D048Q09',\n",
       "  'QTYPE': '',\n",
       "  'ATYPE_': 'Object',\n",
       "  'AMODE_': ['Single-Span-Extraction'],\n",
       "  'QTEXT': '?',\n",
       "  'QTEXT_CN': '?',\n",
       "  'SENTS': [{'text': '?',\n",
       "    'start': 0,\n",
       "    'end': 20,\n",
       "    'IE': {'NER': [],\n",
       "     'COREF': {},\n",
       "     'RELATION': [],\n",
       "     'TOKEN': [{'word': '', 'char_b': 0, 'char_e': 2, 'pos': 'NN'},\n",
       "      {'word': '', 'char_b': 2, 'char_e': 5, 'pos': 'NN'},\n",
       "      {'word': '', 'char_b': 5, 'char_e': 7, 'pos': 'VV'},\n",
       "      {'word': '', 'char_b': 7, 'char_e': 9, 'pos': 'DT'},\n",
       "      {'word': '', 'char_b': 9, 'char_e': 11, 'pos': 'NN'},\n",
       "      {'word': '', 'char_b': 11, 'char_e': 12, 'pos': 'VV'},\n",
       "      {'word': '', 'char_b': 12, 'char_e': 14, 'pos': 'VV'},\n",
       "      {'word': '', 'char_b': 14, 'char_e': 17, 'pos': 'AD'},\n",
       "      {'word': '', 'char_b': 17, 'char_e': 19, 'pos': 'VA'},\n",
       "      {'word': '?', 'char_b': 19, 'char_e': 20, 'pos': 'PU'}]}}],\n",
       "  'SHINT_': [],\n",
       "  'ANSWER': [{'ATEXT': '',\n",
       "    'ATEXT_CN': '',\n",
       "    'ATOKEN': [{'text': '', 'text_cn': '', 'start': 0, 'end': 0}]}]}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_with_performance[training_data_with_performance['train_pred'] != training_data_with_performance['train_obs']]['QUESTIONS'][156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'171\\n791294571()\\n(98914)20,5856.5%(2019)3771(28)D68A6A10(2)A9B511(1)A713157120162018\\n812()'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_with_performance.loc[874]['DTEXT_CN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_with_performance.loc[731]['DTEXT_CN']#[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#20 (Pred:[9], Actual:[])\n",
    "\n",
    "Question:? <br>\n",
    "Predicted SP: <br>\n",
    "Actual: None\n",
    "\n",
    "Comment: While the predicted SP is incorrect, I found out that there is supporting evidence in the paragraph. (...)This might be a case of incorrect input.\n",
    "\n",
    "#70 (Pred:[11], Actual:[])\n",
    "\n",
    "Question: ? <br>\n",
    "Predicted SP: <br>\n",
    "Actual: None\n",
    "\n",
    "Comment: Same as #20. (1992812...199411)\n",
    "\n",
    "#156 (Pred:[1], Actual:[])\n",
    "\n",
    "Question: ? <br>\n",
    "Predicted SP: MIT1966Eliza <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Same ()\n",
    "\n",
    "#284 (Pred:[3], Actual:[])\n",
    "\n",
    "Question:  <br>\n",
    "Predicted SP:  <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Same ()\n",
    "\n",
    "#324 (Pred:[4], Actual:[])\n",
    "\n",
    "Question: ? <br>\n",
    "Predicted SP: <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: No SP in the paragraph. But I think SP is pretty close to being a supporting evidence. This \n",
    "must be a borderline case.\n",
    "\n",
    "#370 (Pred:[12,25], Actual:[25])\n",
    "\n",
    "Question: <br>\n",
    "Predicted SP:  AND 2013<br>\n",
    "Actual: <br>\n",
    "\n",
    "Comment: I think this is a very reasonable mismatch. As two supporting evidences are very similar\n",
    "syntax-wise but drastically different in meaning.\n",
    "\n",
    "#371 (Pred:[12,25], Actual:[12])\n",
    "\n",
    "Question:  <br>\n",
    "Predicted SP:  AND 2013<br>\n",
    "Actual: <br>\n",
    "\n",
    "Comment: Same as #370\n",
    "\n",
    "#395 (Pred:[10], Actual:[])\n",
    "\n",
    "Question: ? <br>\n",
    "Predicted SP:  <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Another potential case of incorrect input. In the paragraph I found this sentence (...)\n",
    "\n",
    "#449 (Pred:[2], Actual:[])\n",
    "\n",
    "Question:  <br>\n",
    "Predicted SP: 24 <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Another similar case. This time I strongly believe this is an incorrect input. \n",
    "7 <- This is sufficient to be a supporting evidence\n",
    "\n",
    "#502 (Pred:[7], Actual:[])\n",
    "\n",
    "Question: ? <br>\n",
    "Predicted SP:  <br>\n",
    "Actual: None \n",
    "\n",
    "Comment: Another similar case. ()\n",
    "\n",
    "#630 (Pred:[62], Actual:[])\n",
    "\n",
    "Question:  <br>\n",
    "Predicted SP: <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Again, I think this counts as a supporting evidence ()\n",
    "\n",
    "#731 (Pred:[9], Actual:[])\n",
    "\n",
    "Question: ? <br>\n",
    "Predicted SP:  <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: I think this counts ()\n",
    "\n",
    "#874 (Pred:[22], Actual:[])\n",
    "\n",
    "Question: \n",
    "Predicted SP: (2019)37\n",
    "Actual: None\n",
    "\n",
    "Comment: No doubt, these are supporting evidences ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
