{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 08:25:38.470734 140283865257792 file_utils.py:39] PyTorch version 1.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence \n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import AdamW\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_json(\"FGC_release_1.7.13/FGC_release_all_dev.json\")\n",
    "training_data = pd.read_json(\"FGC_release_1.7.13/FGC_release_all_train.json\")\n",
    "test_data = pd.read_json(\"FGC_release_1.7.13/FGC_release_all_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocessing(data, return_df=False):\n",
    "    \n",
    "    # Save all the questions, potential supporting evidence and indices in three lists\n",
    "    textQ_to_be_tokenized = []\n",
    "    textA_to_be_tokenized = []\n",
    "    sp_index = []\n",
    "    max_counter = 0\n",
    "    for dictionary in data['QUESTIONS']:\n",
    "        for element in dictionary:\n",
    "            textQ_to_be_tokenized.append(element['QTEXT_CN'])\n",
    "            sp_index.append(element['SHINT_'])\n",
    "    for dictionary in data['SENTS']:\n",
    "        current_text_sentence = []\n",
    "        for element in dictionary:\n",
    "            current_text_sentence.append(element['text'])\n",
    "        textA_to_be_tokenized.append(current_text_sentence)\n",
    "    \n",
    "    QandA_label = pd.DataFrame({'Question': textQ_to_be_tokenized,\n",
    "                                'Sentence_List': textA_to_be_tokenized,\n",
    "                                'SE_Index': sp_index,\n",
    "                                'Label': sp_index})\n",
    "\n",
    "    QandA_label['Length'] = QandA_label['Sentence_List'].apply(lambda x: len(x))\n",
    "    QandA_label['SE_Index'] = QandA_label['SE_Index'].apply(lambda x: [0])\n",
    "    QandA_label['SE_Index'] = QandA_label['SE_Index'] * QandA_label['Length']\n",
    "    QandA_label['SE_Index'] = list(zip(QandA_label['SE_Index'], QandA_label['Label']))\n",
    "\n",
    "    # Extract label index\n",
    "    for row in QandA_label['SE_Index']:\n",
    "        for index in row[1]:\n",
    "            row[0][index] = 1\n",
    "        \n",
    "    indexed = [i[0] for i in list(QandA_label['SE_Index'])]\n",
    "    QandA_label['Label'] = indexed\n",
    "\n",
    "    if return_df:\n",
    "        return QandA_label\n",
    "    \n",
    "    Q_and_Sentence_all_Comb = pd.DataFrame({'Question':np.repeat(QandA_label['Question'].values, QandA_label['Sentence_List'].str.len()),\n",
    "                        'Sentence':np.concatenate(QandA_label['Sentence_List'].values)})\n",
    "    Q_and_Sentence_all_Comb['Label'] = QandA_label['Label'].sum()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "            \n",
    "    # Put all question and sentence combination into a list \n",
    "    All_instances = []\n",
    "    for i in range(len(QandA_label)):\n",
    "        for sentence in QandA_label['Sentence_List'][i]:\n",
    "            question_token = tokenizer.tokenize(QandA_label['Question'][i])\n",
    "            sentence_token = tokenizer.tokenize(sentence)\n",
    "            instance = ['[CLS]'] + question_token + ['[SEP]'] + sentence_token + ['[SEP]']\n",
    "            \n",
    "            if len(instance) > 200:\n",
    "                instance = instance[:200]\n",
    "                max_counter += 1\n",
    "            \n",
    "            #instance = instance[:100]\n",
    "            All_instances.append(instance)\n",
    "            \n",
    "    # Convert ids to segment_ids\n",
    "    segment_ids = []\n",
    "    for token in All_instances:\n",
    "        length_of_zeros = token.index('[SEP]') - token.index('[CLS]') + 1\n",
    "        length_of_ones = len(token) - length_of_zeros\n",
    "        zeros_and_ones = [0] * length_of_zeros + [1] * length_of_ones\n",
    "        segment_ids.append(zeros_and_ones)\n",
    "        \n",
    "    ids = []\n",
    "    for token in All_instances:\n",
    "        ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "        \n",
    "    mask_ids = []\n",
    "    for token in All_instances:\n",
    "        mask_ids.append([1] * len(token))\n",
    "        \n",
    "    labels = list(Q_and_Sentence_all_Comb['Label'])\n",
    "    labels = [[i] for i in labels]\n",
    "    return All_instances, ids, segment_ids, mask_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef window_sentence_preprocessing(data, all_instances, labs, number_of_sentences):\\n        \\n    len_array = np.cumsum(np.array(data['SENTS'].apply(lambda x: len(x))))\\n    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\\n\\n    ids = []\\n    segment_ids = []\\n    mask_ids = []\\n    labels = []\\n    ids_subunit = []\\n    segment_ids_subunit = []\\n    mask_ids_subunit = []\\n    labels_subunit = []\\n    unit_counter = 0\\n    \\n    for count, token in enumerate(all_instances, 1):\\n        # Append ids\\n        ids_subunit.append(tokenizer.convert_tokens_to_ids(token))\\n        \\n        # Append segment ids\\n        length_of_zeros = token.index('[SEP]') - token.index('[CLS]') + 1\\n        length_of_ones = len(token) - length_of_zeros\\n        zeros_and_ones = [0] * length_of_zeros + [1] * length_of_ones\\n        segment_ids_subunit.append(zeros_and_ones)\\n        \\n        # Append mask ids\\n        mask_ids_subunit.append([1] * len(token))\\n        \\n        unit_counter += 1\\n        \\n        if bool(unit_counter % number_of_sentences == 0) or bool(count in len_array):\\n            \\n            ids.append(ids_subunit)\\n            segment_ids.append(segment_ids_subunit)\\n            mask_ids.append(mask_ids_subunit)\\n            \\n            ids_subunit = []\\n            segment_ids_subunit = []\\n            mask_ids_subunit = []\\n            unit_counter = 0\\n    \\n    label_counter = 0\\n    # Append labels\\n    for count, label in enumerate(labs, 1):\\n        \\n        labels_subunit.append(label)\\n        label_counter += 1\\n        \\n        if bool(label_counter % number_of_sentences == 0) or bool(count in len_array): \\n            \\n            labels.append(labels_subunit)\\n            labels_subunit = []\\n            label_counter = 0\\n    \\n            \\n    return ids, segment_ids, mask_ids, labels\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def window_sentence_preprocessing(data, all_instances, labs, number_of_sentences):\n",
    "        \n",
    "    len_array = np.cumsum(np.array(data['SENTS'].apply(lambda x: len(x))))\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "    ids = []\n",
    "    segment_ids = []\n",
    "    mask_ids = []\n",
    "    labels = []\n",
    "    ids_subunit = []\n",
    "    segment_ids_subunit = []\n",
    "    mask_ids_subunit = []\n",
    "    labels_subunit = []\n",
    "    unit_counter = 0\n",
    "    \n",
    "    for count, token in enumerate(all_instances, 1):\n",
    "        # Append ids\n",
    "        ids_subunit.append(tokenizer.convert_tokens_to_ids(token))\n",
    "        \n",
    "        # Append segment ids\n",
    "        length_of_zeros = token.index('[SEP]') - token.index('[CLS]') + 1\n",
    "        length_of_ones = len(token) - length_of_zeros\n",
    "        zeros_and_ones = [0] * length_of_zeros + [1] * length_of_ones\n",
    "        segment_ids_subunit.append(zeros_and_ones)\n",
    "        \n",
    "        # Append mask ids\n",
    "        mask_ids_subunit.append([1] * len(token))\n",
    "        \n",
    "        unit_counter += 1\n",
    "        \n",
    "        if bool(unit_counter % number_of_sentences == 0) or bool(count in len_array):\n",
    "            \n",
    "            ids.append(ids_subunit)\n",
    "            segment_ids.append(segment_ids_subunit)\n",
    "            mask_ids.append(mask_ids_subunit)\n",
    "            \n",
    "            ids_subunit = []\n",
    "            segment_ids_subunit = []\n",
    "            mask_ids_subunit = []\n",
    "            unit_counter = 0\n",
    "    \n",
    "    label_counter = 0\n",
    "    # Append labels\n",
    "    for count, label in enumerate(labs, 1):\n",
    "        \n",
    "        labels_subunit.append(label)\n",
    "        label_counter += 1\n",
    "        \n",
    "        if bool(label_counter % number_of_sentences == 0) or bool(count in len_array): \n",
    "            \n",
    "            labels.append(labels_subunit)\n",
    "            labels_subunit = []\n",
    "            label_counter = 0\n",
    "    \n",
    "            \n",
    "    return ids, segment_ids, mask_ids, labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def window_sentence_preprocessing(data, dataset, number_of_sentences):\n",
    "    \n",
    "    len_array = np.cumsum(np.array(data['SENTS'].apply(lambda x: len(x))))\n",
    "    dictionary_lists = []\n",
    "    batches = []\n",
    "    \n",
    "    unit_counter = 0\n",
    "    for count, instance in enumerate(dataset, 1):\n",
    "        \n",
    "        dictionary_lists.append(instance)\n",
    "        unit_counter += 1\n",
    "        \n",
    "        if (unit_counter % number_of_sentences == 0) or (count in len_array):\n",
    "            \n",
    "            padded_ids = pad_sequence([torch.tensor(instance['ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_ids = padded_ids.to(device)\n",
    "\n",
    "            padded_segment_ids = pad_sequence([torch.tensor(instance['segment_ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_segment_ids = padded_segment_ids.to(device)\n",
    "\n",
    "            padded_mask_ids = pad_sequence([torch.tensor(instance['mask_ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_mask_ids = padded_mask_ids.to(device)\n",
    "\n",
    "            labels = torch.stack([torch.tensor(instance['labels']) for instance in dictionary_lists])\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            current_dev_batch = {'ids': padded_ids, 'mask_ids': padded_mask_ids, 'segment_ids': padded_segment_ids, 'labels': labels}\n",
    "\n",
    "            batches.append(current_dev_batch)\n",
    "            dictionary_lists = []\n",
    "            unit_counter = 0\n",
    "    \n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocessing(data, dataset):\n",
    "    \n",
    "    len_array = np.cumsum(np.array(data['SENTS'].apply(lambda x: len(x))))\n",
    "\n",
    "    dictionary_lists = []\n",
    "    batches = []\n",
    "    for i in range(len(dataset.instances)):\n",
    "\n",
    "        instance = dataset.instances[i]\n",
    "        dictionary_lists.append(instance)\n",
    "        \n",
    "        if i in len_array - 1:\n",
    "\n",
    "            padded_ids = pad_sequence([torch.tensor(instance['ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_ids = padded_ids.to(device)\n",
    "\n",
    "            padded_segment_ids = pad_sequence([torch.tensor(instance['segment_ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_segment_ids = padded_segment_ids.to(device)\n",
    "\n",
    "            padded_mask_ids = pad_sequence([torch.tensor(instance['mask_ids']) for instance in dictionary_lists], batch_first=True)\n",
    "            padded_mask_ids = padded_mask_ids.to(device)\n",
    "\n",
    "            labels = torch.stack([torch.tensor(instance['labels']) for instance in dictionary_lists])\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            current_dev_batch = {'ids': padded_ids, 'mask_ids': padded_mask_ids, 'segment_ids': padded_segment_ids, 'labels': labels}\n",
    "\n",
    "            batches.append(current_dev_batch)\n",
    "            dictionary_lists = []\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 08:25:42.386647 140283865257792 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "I0723 08:25:47.220201 140283865257792 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "I0723 08:26:05.825831 140283865257792 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "dev_instances, dev_ids, dev_seg_ids, dev_mask_ids, dev_labels = datapreprocessing(validation_data)\n",
    "train_instances, train_ids, train_seg_ids, train_mask_ids, train_labels = datapreprocessing(training_data)\n",
    "test_instances, test_ids, test_seg_ids, test_mask_ids, test_labels = datapreprocessing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ids, segment_ids, mask_ids, labels):\n",
    "        self.instances = []\n",
    "        for ids_i, segment_ids_i, mask_ids, label in zip(ids, segment_ids, mask_ids, labels):\n",
    "            self.instances.append({\"ids\": ids_i, \"segment_ids\": segment_ids_i, \n",
    "                                   \"mask_ids\": mask_ids, \"labels\": label})  \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.instances[idx]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentenceDataset(train_ids, train_seg_ids, train_mask_ids, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = SentenceDataset(dev_ids, dev_seg_ids, dev_mask_ids, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SentenceDataset(test_ids, test_seg_ids, test_mask_ids, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    padded_ids = pad_sequence([torch.tensor(instance['ids']) for instance in batch], batch_first=True)\n",
    "    padded_ids = padded_ids.to(device)\n",
    "    \n",
    "    padded_segment_ids = pad_sequence([torch.tensor(instance['segment_ids']) for instance in batch], batch_first=True)\n",
    "    padded_segment_ids = padded_segment_ids.to(device)\n",
    "    \n",
    "    padded_mask_ids = pad_sequence([torch.tensor(instance['mask_ids']) for instance in batch], batch_first=True)\n",
    "    padded_mask_ids = padded_mask_ids.to(device)\n",
    "    \n",
    "    labels = torch.stack([torch.tensor(instance['labels']) for instance in batch])\n",
    "    labels = labels.to(device)\n",
    "    return {'ids': padded_ids, 'mask_ids': padded_mask_ids, 'segment_ids': padded_segment_ids, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_3d(batch):\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    max_num_sentences, max_sentence_length = find_max_dimension(batch)\n",
    "    target_ids = torch.zeros(batch_size, max_num_sentences, max_sentence_length)\n",
    "    target_segment_ids = torch.zeros(batch_size, max_num_sentences, max_sentence_length)\n",
    "    target_mask_ids = torch.zeros(batch_size, max_num_sentences, max_sentence_length)\n",
    "    target_labels = torch.zeros(batch_size, max_num_sentences, 1)\n",
    "    for i in range(len(batch)):\n",
    "        \n",
    "        source_id_dimension = batch[i]['ids'].shape\n",
    "        target_ids[i, :source_id_dimension[0], :source_id_dimension[1]] = batch[i]['ids']\n",
    "        \n",
    "        source_segment_id_dimension = batch[i]['segment_ids'].shape\n",
    "        target_segment_ids[i, :source_segment_id_dimension[0], :source_segment_id_dimension[1]] = batch[i]['segment_ids']\n",
    "        \n",
    "        source_mask_id_dimension = batch[i]['mask_ids'].shape\n",
    "        target_mask_ids[i, :source_mask_id_dimension[0], :source_mask_id_dimension[1]] = batch[i]['mask_ids']\n",
    "        \n",
    "        source_label_dimension = batch[i]['labels'].shape\n",
    "        target_labels[i, :source_label_dimension[0], :source_label_dimension[1]] = batch[i]['labels']\n",
    "    \n",
    "    target_ids = target_ids.to(device).to(torch.long)\n",
    "    target_segment_ids = target_segment_ids.to(device).to(torch.long)\n",
    "    target_mask_ids = target_segment_ids.to(device).to(torch.long)\n",
    "    target_labels = target_labels.to(device).to(torch.long)\n",
    "    \n",
    "    return {'ids': target_ids, 'mask_ids': target_mask_ids, 'segment_ids': target_segment_ids, 'labels': target_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dimension(batch):\n",
    "    num_sentences = []\n",
    "    sentence_lengths = []\n",
    "    for question in batch:\n",
    "        num_sentences.append(question['ids'].shape[0])\n",
    "        sentence_lengths.append(question['ids'].shape[1])\n",
    "    return max(num_sentences), max(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_batches = window_sentence_preprocessing(training_data, train_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(train_dataset, batch_size=8, shuffle = True, collate_fn = collate)\n",
    "dataloader_train_3d = DataLoader(new_train_batches, batch_size=2, shuffle = True, collate_fn = collate_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_lengths = []\n",
    "dev_sentence_lengths = []\n",
    "test_sentence_lengths = []\n",
    "\n",
    "for instance_dict in train_dataset.instances:\n",
    "    train_sentence_lengths.append(len(instance_dict['ids']))\n",
    "    \n",
    "for instance_dict in dev_dataset.instances:\n",
    "    dev_sentence_lengths.append(len(instance_dict['ids']))\n",
    "    \n",
    "for instance_dict in test_dataset.instances:\n",
    "    test_sentence_lengths.append(len(instance_dict['ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   3.,  50., 138., 281., 531., 595., 299., 722., 794., 669.,\n",
       "        646., 536., 499., 449., 192., 340., 305., 242., 229., 212., 137.,\n",
       "        115.,  61., 103.,  96.,  47.,  53.,  55.,  35.,  19.,  41.,  37.,\n",
       "         43.,  34.,  26.,  26.,  18.,  12.,  14.,  18.,  12.,  18.,   8.,\n",
       "          4.,   5.,   2.,   3.,   4.,   5.,   5.,   1.,   2.,   3.,   4.,\n",
       "          2.,   6.,   8.,   2.,   5.,   0.,   0.,   1.,   2.,   0.,   0.,\n",
       "          2.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         12.]),\n",
       " array([ 13.  ,  14.87,  16.74,  18.61,  20.48,  22.35,  24.22,  26.09,\n",
       "         27.96,  29.83,  31.7 ,  33.57,  35.44,  37.31,  39.18,  41.05,\n",
       "         42.92,  44.79,  46.66,  48.53,  50.4 ,  52.27,  54.14,  56.01,\n",
       "         57.88,  59.75,  61.62,  63.49,  65.36,  67.23,  69.1 ,  70.97,\n",
       "         72.84,  74.71,  76.58,  78.45,  80.32,  82.19,  84.06,  85.93,\n",
       "         87.8 ,  89.67,  91.54,  93.41,  95.28,  97.15,  99.02, 100.89,\n",
       "        102.76, 104.63, 106.5 , 108.37, 110.24, 112.11, 113.98, 115.85,\n",
       "        117.72, 119.59, 121.46, 123.33, 125.2 , 127.07, 128.94, 130.81,\n",
       "        132.68, 134.55, 136.42, 138.29, 140.16, 142.03, 143.9 , 145.77,\n",
       "        147.64, 149.51, 151.38, 153.25, 155.12, 156.99, 158.86, 160.73,\n",
       "        162.6 , 164.47, 166.34, 168.21, 170.08, 171.95, 173.82, 175.69,\n",
       "        177.56, 179.43, 181.3 , 183.17, 185.04, 186.91, 188.78, 190.65,\n",
       "        192.52, 194.39, 196.26, 198.13, 200.  ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(dev_sentence_lengths, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Baseline Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGC_Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(FGC_Network, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # batch['ids'] = (batch_size, sent_len)\n",
    "        # batch['segment_ids'] = (batch_size, sent_len)\n",
    "        # batch['mask_ids'] = = (batch_size, sent_len)\n",
    "        # output = (batch_size, 1)\n",
    "        hidden_state, pooler_output = self.bert(batch['ids'], batch['mask_ids'], batch['segment_ids'])\n",
    "        linear_output = self.linear(pooler_output)\n",
    "        \n",
    "        return linear_output\n",
    "\n",
    "    def loss(self, batch):\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        output = self.forward(batch)\n",
    "        target = batch['labels'].float().to(device)\n",
    "        \n",
    "        return loss_fn(output, target)\n",
    "    \n",
    "    def _predict(self, batch):\n",
    "        \n",
    "        output = self.forward(batch)\n",
    "        scores = torch.sigmoid(output)\n",
    "        scores = scores.cpu().numpy()[:,0].tolist()\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict_fgc(self, batch, threshold=0.5):\n",
    "        \n",
    "        scores = self._predict(batch)\n",
    "        max_i = 0\n",
    "        max_score = 0\n",
    "        sp = []\n",
    "        \n",
    "        for i, score in enumerate(scores):\n",
    "\n",
    "            if score > max_score:\n",
    "                max_i = i\n",
    "                max_score = score\n",
    "            if score >= threshold:\n",
    "                sp.append(i)\n",
    "\n",
    "        if not sp:\n",
    "            sp.append(max_i)\n",
    "\n",
    "        return {'sp': sp, 'sp_scores': scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FGC_Network()\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Baseline Model & Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim(nn, num_epochs, lr):\n",
    "    param_optimizer = list(nn.bert.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_epochs = num_epochs\n",
    "    num_train_optimization_steps = len(dataloader_train) * num_epochs\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                     num_warmup_steps=int(\n",
    "                                                         num_train_optimization_steps * 0.1),\n",
    "                                                     num_training_steps=num_train_optimization_steps)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_sp(metrics, sp_gold, sp_pred):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "        \n",
    "    for p in sp_pred:\n",
    "        if p in sp_gold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for g in sp_gold:\n",
    "        if g not in sp_pred:\n",
    "            fn += 1\n",
    "            \n",
    "    precision = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "    \n",
    "    metrics['sp_em'] += em\n",
    "    metrics['sp_f1'] += f1\n",
    "    metrics['sp_prec'] += precision\n",
    "    metrics['sp_recall'] += recall\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sp_fgc(sp_golds, sp_preds):\n",
    "    \n",
    "    metrics = {'sp_em': 0, 'sp_prec': 0, 'sp_recall': 0, 'sp_f1': 0}\n",
    "    \n",
    "    assert len(sp_golds) == len(sp_preds)\n",
    "    \n",
    "    for sp_gold, sp_pred in zip(sp_golds, sp_preds):\n",
    "        _update_sp(metrics, sp_gold, sp_pred)\n",
    "    \n",
    "    N = len(sp_golds)\n",
    "    for k in metrics.keys():\n",
    "        metrics[k] /= N\n",
    "        metrics[k] = round(metrics[k], 3)\n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fgc_atype(atype_golds, atype_preds):\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    for gold, atype in zip(atype_golds, atype_preds):\n",
    "        if atype == gold:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    return pos/len(atypes_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(network, dev_batches, current_epoch, sp_golds, avg_loss):\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sp_preds = []\n",
    "        \n",
    "        for batch in tqdm(dev_batches):\n",
    "            \n",
    "            out_dct = network.predict_fgc(batch)\n",
    "            sp_preds.append(out_dct['sp'])\n",
    "                \n",
    "              \n",
    "    metrics = eval_sp_fgc(sp_golds, sp_preds)\n",
    "    print('epoch %d eval_recall: %.3f eval_f1: %.3f loss: %.3f' % (\n",
    "            current_epoch, metrics['sp_recall'], metrics['sp_f1'], avg_loss))\n",
    "        \n",
    "    torch.save(network.state_dict(), \"FGC_release_1.7.13/New_Models/model_epoch{0}_eval_em:{1:.3f}_precision:{2:.3f}_recall:{3:.3f}_f1:{4:.3f}_loss:{5:.3f}.m\".format(current_epoch, metrics['sp_em'], metrics['sp_prec'], metrics['sp_recall'], metrics['sp_f1'], avg_loss))\n",
    "    \n",
    "    return sp_preds, sp_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, data, dev_batches, num_epochs, lr):\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer, scheduler = optim(network, num_epochs, lr)\n",
    "    \n",
    "    sp_golds = validation_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist()\n",
    "    \n",
    "    for current_epoch in range(num_epochs):\n",
    "        network.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(data):\n",
    "            optimizer.zero_grad()\n",
    "            current_output = network(batch)\n",
    "            current_target = batch['labels'].to(dtype=torch.float, device=device)\n",
    "            current_loss = loss_fn(current_output, current_target)\n",
    "\n",
    "            current_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(network.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += current_loss.item()\n",
    "            \n",
    "        learning_rate_scalar = scheduler.get_lr()[0]\n",
    "        print('lr = %f' % learning_rate_scalar)\n",
    "        avg_loss = running_loss/len(data)\n",
    "        print('epoch %d train_loss: %.3f' % (current_epoch, avg_loss))\n",
    "        eval(network, data, dev_batches, current_epoch, sp_golds, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(network, 20, 0.00002) # if you want to run this again, rememebr to add the parameter 'batches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_network = FGC_Network()\n",
    "trained_network.load_state_dict(torch.load(\"Models/baseline_models_with_scheduler/model_epoch8_eval_em:0.198_precision:0.603_recall:0.588_f1:0.545_loss:0.031.m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708e6b9c49844d6c86c68ac54eb08620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=882), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.902, 'sp_prec': 0.978, 'sp_recall': 0.963, 'sp_f1': 0.966}\n",
      "epoch 0 eval_recall: 0.963 eval_f1: 0.966 loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "sp_golds = training_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist()\n",
    "batches = eval_preprocessing(training_data, train_dataset)\n",
    "\n",
    "trained_network.to(\"cuda\")\n",
    "train_pred, train_obs = eval(trained_network, batches, 0, sp_golds, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672c4f1229f942e791b8dcc953b89690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.198, 'sp_prec': 0.603, 'sp_recall': 0.588, 'sp_f1': 0.545}\n",
      "epoch 0 eval_recall: 0.588 eval_f1: 0.545 loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "sp_golds = validation_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist()\n",
    "dev_batches = eval_preprocessing(validation_data, dev_dataset)\n",
    "dev_preds, dev_obs = eval(trained_network, dev_batches, 0, sp_golds, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63425e216ff14a6fa9b6998e6cf21665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=193), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.233, 'sp_prec': 0.661, 'sp_recall': 0.573, 'sp_f1': 0.57}\n",
      "epoch 0 eval_recall: 0.573 eval_f1: 0.570 loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "sp_golds = test_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist()\n",
    "test_batches = eval_preprocessing(test_data, test_dataset)\n",
    "test_preds, test_obds = eval(trained_network, test_batches, 0, sp_golds, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_with_performance = datapreprocessing(validation_data, True)\n",
    "training_data_with_performance = datapreprocessing(training_data, True)\n",
    "test_data_with_performance = datapreprocessing(test_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_with_performance['train_pred'] = train_pred\n",
    "training_data_with_performance['train_obs'] = train_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sp = []\n",
    "for i in range(training_data_with_performance.shape[0]):\n",
    "    para = training_data_with_performance['Sentence_List'][i]\n",
    "    sen = []\n",
    "    for index in training_data_with_performance['train_pred'][i]:\n",
    "        sen.append(para[index])\n",
    "    correct_sp.append(sen)\n",
    "training_data_with_performance['Pred_List'] = correct_sp\n",
    "correct_sp = []\n",
    "for i in range(training_data_with_performance.shape[0]):\n",
    "    para = training_data_with_performance['Sentence_List'][i]\n",
    "    sen = []\n",
    "    for index in training_data_with_performance['train_obs'][i]:\n",
    "        sen.append(para[index])\n",
    "    correct_sp.append(sen)\n",
    "training_data_with_performance['Obs_List'] = correct_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_with_performance.drop(['SE_Index', 'Label', 'train_pred', 'train_obs'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Sentence_List</th>\n",
       "      <th>Length</th>\n",
       "      <th>Pred_List</th>\n",
       "      <th>Obs_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>苏东坡出生于哪一年?</td>\n",
       "      <td>[嘉佑二年（1057年），, 苏轼才20岁，, 与弟弟苏辙一同进京参加会考，, 苏轼中进士第...</td>\n",
       "      <td>35</td>\n",
       "      <td>[苏轼才20岁，]</td>\n",
       "      <td>[嘉佑二年（1057年），, 苏轼才20岁，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>苏东坡和谁一起进京参加会考?</td>\n",
       "      <td>[嘉佑二年（1057年），, 苏轼才20岁，, 与弟弟苏辙一同进京参加会考，, 苏轼中进士第...</td>\n",
       "      <td>35</td>\n",
       "      <td>[与弟弟苏辙一同进京参加会考，]</td>\n",
       "      <td>[苏轼才20岁，, 与弟弟苏辙一同进京参加会考，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>在苏东坡被王安石诬陷时，谁为他说话？</td>\n",
       "      <td>[嘉佑二年（1057年），, 苏轼才20岁，, 与弟弟苏辙一同进京参加会考，, 苏轼中进士第...</td>\n",
       "      <td>35</td>\n",
       "      <td>[范镇极辩苏轼贩盐之诬，]</td>\n",
       "      <td>[范镇极辩苏轼贩盐之诬，, 并愿意退休负责。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>苏东坡哪一年离开海南岛?</td>\n",
       "      <td>[元祐元年（1086年），, 宋哲宗即位，, 高太皇太后垂帘听政，, 回朝任礼部郎中、中书舍...</td>\n",
       "      <td>32</td>\n",
       "      <td>[\\n绍圣元年（1094年）被哲宗贬谪至惠州、儋州（海南岛）。, 下诏让苏轼北还。]</td>\n",
       "      <td>[\\n绍圣元年（1094年）被哲宗贬谪至惠州、儋州（海南岛）。, \\n元符三年（1100年）...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>苏东坡死于哪一年?</td>\n",
       "      <td>[元祐元年（1086年），, 宋哲宗即位，, 高太皇太后垂帘听政，, 回朝任礼部郎中、中书舍...</td>\n",
       "      <td>32</td>\n",
       "      <td>[七月二十八日于常州孙氏馆病卒，]</td>\n",
       "      <td>[\\n建中靖国元年（1101年），, 七月二十八日于常州孙氏馆病卒，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>青河大学时代在哪一所学校念书?</td>\n",
       "      <td>[为倾听新住民心声，, 内政部移民署今(18)日下午，, 在台南市举办新住民座谈会，, 有2...</td>\n",
       "      <td>56</td>\n",
       "      <td>[青河毕业于越南河内国家大学法文系，, 青河得以持续进修，]</td>\n",
       "      <td>[青河毕业于越南河内国家大学法文系，, 青河得以持续进修，, 她不仅取得国立成功大学硕士学位，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>青河的硕士学位在哪一个国家念的?</td>\n",
       "      <td>[为倾听新住民心声，, 内政部移民署今(18)日下午，, 在台南市举办新住民座谈会，, 有2...</td>\n",
       "      <td>56</td>\n",
       "      <td>[青河毕业于越南河内国家大学法文系，, 青河得以持续进修，]</td>\n",
       "      <td>[青河毕业于越南河内国家大学法文系，, 青河得以持续进修，, 她不仅取得国立成功大学硕士学位，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>如果我使用了日本买的眼药水却发生过敏红肿，是否可以申请药害救济?</td>\n",
       "      <td>[「目睭花花，匏仔看做菜瓜」，, 许多民众如果觉得眼睛雾雾时，, 常常会自行购买眼药水来缓解...</td>\n",
       "      <td>39</td>\n",
       "      <td>[透过网购或国外带回的眼药水，, 并不属于我国合法药物，, 并不适用我国药害救济制度喔!\\n...</td>\n",
       "      <td>[透过网购或国外带回的眼药水，, 并不适用我国药害救济制度喔!\\n\\n       食药署为...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>目前在台湾出现的肠病毒有哪几型?</td>\n",
       "      <td>[国内肠病毒轻症疫情持续上升，, 另新增1例肠病毒71型并发重症病例。, 疾病管制署再次呼吁...</td>\n",
       "      <td>42</td>\n",
       "      <td>[以感染肠病毒71型为多(28例)，, 其他分别感染肠病毒D68型、克沙奇A6型、A10型(...</td>\n",
       "      <td>[今(2019)年累计37例肠病毒并发重症病例，, 以感染肠病毒71型为多(28例)，, 其...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>要如何降低肠病毒的传播风险？</td>\n",
       "      <td>[国内肠病毒轻症疫情持续上升，, 另新增1例肠病毒71型并发重症病例。, 疾病管制署再次呼吁...</td>\n",
       "      <td>42</td>\n",
       "      <td>[疾病管制署再次呼吁，]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Question  \\\n",
       "1                          苏东坡出生于哪一年?   \n",
       "2                      苏东坡和谁一起进京参加会考?   \n",
       "8                  在苏东坡被王安石诬陷时，谁为他说话？   \n",
       "12                       苏东坡哪一年离开海南岛?   \n",
       "13                          苏东坡死于哪一年?   \n",
       "..                                ...   \n",
       "829                  青河大学时代在哪一所学校念书?    \n",
       "831                  青河的硕士学位在哪一个国家念的?   \n",
       "867  如果我使用了日本买的眼药水却发生过敏红肿，是否可以申请药害救济?   \n",
       "870                  目前在台湾出现的肠病毒有哪几型?   \n",
       "874                    要如何降低肠病毒的传播风险？   \n",
       "\n",
       "                                         Sentence_List  Length  \\\n",
       "1    [嘉佑二年（1057年），, 苏轼才20岁，, 与弟弟苏辙一同进京参加会考，, 苏轼中进士第...      35   \n",
       "2    [嘉佑二年（1057年），, 苏轼才20岁，, 与弟弟苏辙一同进京参加会考，, 苏轼中进士第...      35   \n",
       "8    [嘉佑二年（1057年），, 苏轼才20岁，, 与弟弟苏辙一同进京参加会考，, 苏轼中进士第...      35   \n",
       "12   [元祐元年（1086年），, 宋哲宗即位，, 高太皇太后垂帘听政，, 回朝任礼部郎中、中书舍...      32   \n",
       "13   [元祐元年（1086年），, 宋哲宗即位，, 高太皇太后垂帘听政，, 回朝任礼部郎中、中书舍...      32   \n",
       "..                                                 ...     ...   \n",
       "829  [为倾听新住民心声，, 内政部移民署今(18)日下午，, 在台南市举办新住民座谈会，, 有2...      56   \n",
       "831  [为倾听新住民心声，, 内政部移民署今(18)日下午，, 在台南市举办新住民座谈会，, 有2...      56   \n",
       "867  [「目睭花花，匏仔看做菜瓜」，, 许多民众如果觉得眼睛雾雾时，, 常常会自行购买眼药水来缓解...      39   \n",
       "870  [国内肠病毒轻症疫情持续上升，, 另新增1例肠病毒71型并发重症病例。, 疾病管制署再次呼吁...      42   \n",
       "874  [国内肠病毒轻症疫情持续上升，, 另新增1例肠病毒71型并发重症病例。, 疾病管制署再次呼吁...      42   \n",
       "\n",
       "                                             Pred_List  \\\n",
       "1                                            [苏轼才20岁，]   \n",
       "2                                     [与弟弟苏辙一同进京参加会考，]   \n",
       "8                                        [范镇极辩苏轼贩盐之诬，]   \n",
       "12          [\\n绍圣元年（1094年）被哲宗贬谪至惠州、儋州（海南岛）。, 下诏让苏轼北还。]   \n",
       "13                                   [七月二十八日于常州孙氏馆病卒，]   \n",
       "..                                                 ...   \n",
       "829                     [青河毕业于越南河内国家大学法文系，, 青河得以持续进修，]   \n",
       "831                     [青河毕业于越南河内国家大学法文系，, 青河得以持续进修，]   \n",
       "867  [透过网购或国外带回的眼药水，, 并不属于我国合法药物，, 并不适用我国药害救济制度喔!\\n...   \n",
       "870  [以感染肠病毒71型为多(28例)，, 其他分别感染肠病毒D68型、克沙奇A6型、A10型(...   \n",
       "874                                       [疾病管制署再次呼吁，]   \n",
       "\n",
       "                                              Obs_List  \n",
       "1                              [嘉佑二年（1057年），, 苏轼才20岁，]  \n",
       "2                            [苏轼才20岁，, 与弟弟苏辙一同进京参加会考，]  \n",
       "8                              [范镇极辩苏轼贩盐之诬，, 并愿意退休负责。]  \n",
       "12   [\\n绍圣元年（1094年）被哲宗贬谪至惠州、儋州（海南岛）。, \\n元符三年（1100年）...  \n",
       "13                 [\\n建中靖国元年（1101年），, 七月二十八日于常州孙氏馆病卒，]  \n",
       "..                                                 ...  \n",
       "829   [青河毕业于越南河内国家大学法文系，, 青河得以持续进修，, 她不仅取得国立成功大学硕士学位，]  \n",
       "831   [青河毕业于越南河内国家大学法文系，, 青河得以持续进修，, 她不仅取得国立成功大学硕士学位，]  \n",
       "867  [透过网购或国外带回的眼药水，, 并不适用我国药害救济制度喔!\\n\\n       食药署为...  \n",
       "870  [今(2019)年累计37例肠病毒并发重症病例，, 以感染肠病毒71型为多(28例)，, 其...  \n",
       "874                                                 []  \n",
       "\n",
       "[86 rows x 5 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mismatch = training_data_with_performance[training_data_with_performance['Pred_List'] != training_data_with_performance['Obs_List']]\n",
    "train_mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#20 (Pred:[9], Actual:[])\n",
    "\n",
    "Question:「阿拉伯之春」运动中，走上街头的民众的诉求为何? <br>\n",
    "Predicted SP: 只有突尼西亚成为阿拉伯之春中，<br>\n",
    "Actual: None\n",
    "\n",
    "Comment: While the predicted SP is incorrect, I found out that there is supporting evidence in the paragraph. (...要求推翻本国的专制政体的行动)This might be a case of incorrect input.\n",
    "\n",
    "#70 (Pred:[11], Actual:[])\n",
    "\n",
    "Question: 第二次签订的北美贸易协定从签署至生效过了几日? <br>\n",
    "Predicted SP: 美国、墨西哥和加拿大就更新北美自由贸易协定达成一致，<br>\n",
    "Actual: None\n",
    "\n",
    "Comment: Same as #20. (美国、加拿大及墨西哥在1992年8月12日签署了关于三国间全面贸易的协议。...，北美自由贸易协议于1994年1月1日正式生效。)\n",
    "\n",
    "#156 (Pred:[1], Actual:[])\n",
    "\n",
    "Question: 聊天机器人仰赖哪些方法让回答愈来愈准确? <br>\n",
    "Predicted SP: 麻省理工学院（MIT）人工智慧实验室早在1966年即研发出名为「Eliza」的机器人， <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Same (聊天机器人的作答准确度要透过程式化的方法改善)\n",
    "\n",
    "#284 (Pred:[3], Actual:[])\n",
    "\n",
    "Question: 不可再生能源的意义是什么？ <br>\n",
    "Predicted SP: 许多这些形式可以很容易转化为另一种的帮助下， <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Same (是无法经过短时间内再生的能源，而且它们的消耗速度远远超过它们再生的速度)\n",
    "\n",
    "#324 (Pred:[4], Actual:[])\n",
    "\n",
    "Question: 伊甸基金會成立的宗旨為何? <br>\n",
    "Predicted SP: 因著上帝的呼召及一颗爱身心障碍者的同理心，<br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: No SP in the paragraph. But I think SP is pretty close to being a supporting evidence. This \n",
    "must be a borderline case.\n",
    "\n",
    "#370 (Pred:[12,25], Actual:[25])\n",
    "\n",
    "Question: 三大健康照护体系保险制度中，政府涉入程度低的是哪一种？<br>\n",
    "Predicted SP: 公医制（政府介入最多）：以英国为代表。 AND 自由市场（政府一般不介入）：以2013年前的美国为代表。<br>\n",
    "Actual: 公医制（政府介入最多）：以英国为代表。<br>\n",
    "\n",
    "Comment: I think this is a very reasonable mismatch. As two supporting evidences are very similar\n",
    "syntax-wise but drastically different in meaning.\n",
    "\n",
    "#371 (Pred:[12,25], Actual:[12])\n",
    "\n",
    "Question: 三大健康照護體系保險制度中，政府涉入程度高的是哪一種？ <br>\n",
    "Predicted SP: 公医制（政府介入最多）：以英国为代表。 AND 自由市场（政府一般不介入）：以2013年前的美国为代表。<br>\n",
    "Actual: 公医制（政府介入最多）：以英国为代表。<br>\n",
    "\n",
    "Comment: Same as #370\n",
    "\n",
    "#395 (Pred:[10], Actual:[])\n",
    "\n",
    "Question: 熬夜是否能减低得到癌症的风险? <br>\n",
    "Predicted SP: 皆强烈建议减少或避免动物性食品摄取， <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Another potential case of incorrect input. In the paragraph I found this sentence (所以防癌守则：...，注重睡眠品质)\n",
    "\n",
    "#449 (Pred:[2], Actual:[])\n",
    "\n",
    "Question: 高屏地区国庆烟火试放管制时间是从晚上几点开始？ <br>\n",
    "Predicted SP: 屏东县政府表示24号当天屏东河滨公园将管制不开放， <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Another similar case. This time I strongly believe this is an incorrect input. \n",
    "当晚7时并会进行全面清场 <- This is sufficient to be a supporting evidence\n",
    "\n",
    "#502 (Pred:[7], Actual:[])\n",
    "\n",
    "Question: 为何圣伯多禄大殿只能重建不能整修就好? <br>\n",
    "Predicted SP: 教宗犹利二世决定重建圣伯多禄大殿 <br>\n",
    "Actual: None \n",
    "\n",
    "Comment: Another similar case. (无疑再改动有机会让建筑倒塌)\n",
    "\n",
    "#630 (Pred:[62], Actual:[])\n",
    "\n",
    "Question: 毛笔、铅笔、钢笔，这三种笔中哪个笔尖的硬度高？ <br>\n",
    "Predicted SP: 更进一步看：我们无论使用那一种笔，<br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: Again, I think this counts as a supporting evidence (钢笔的笔尖用金属制成，弹性大，硬度高)\n",
    "\n",
    "#731 (Pred:[9], Actual:[])\n",
    "\n",
    "Question: 为什么古埃及人要把死人做成木乃伊? <br>\n",
    "Predicted SP: 是做什么用的呢？ <br>\n",
    "Actual: None <br>\n",
    "\n",
    "Comment: I think this counts (古埃及人相信：人死后只要把遗体保存好，就可以在另一个世界得到永生。)\n",
    "\n",
    "#874 (Pred:[22], Actual:[])\n",
    "\n",
    "Question: 要如何降低肠病毒的传播风险？\n",
    "Predicted SP: 今(2019)年累计37例肠病毒并发重症病例，\n",
    "Actual: None\n",
    "\n",
    "Comment: No doubt, these are supporting evidences (应加强居家环境、教室及游乐设施等的通风、整洁与消毒，并教导学童落实「湿、搓、冲、捧、擦」正确洗手步骤，及生病在家休息等良好卫生观念，)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_with_performance['dev_pred'] = dev_preds\n",
    "validation_data_with_performance['dev_obs'] = dev_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sp = []\n",
    "for i in range(validation_data_with_performance.shape[0]):\n",
    "    para = validation_data_with_performance['Sentence_List'][i]\n",
    "    sen = []\n",
    "    for index in validation_data_with_performance['dev_pred'][i]:\n",
    "        sen.append(para[index])\n",
    "    correct_sp.append(sen)\n",
    "validation_data_with_performance['Pred_List'] = correct_sp\n",
    "correct_sp = []\n",
    "for i in range(validation_data_with_performance.shape[0]):\n",
    "    para = validation_data_with_performance['Sentence_List'][i]\n",
    "    sen = []\n",
    "    for index in validation_data_with_performance['dev_obs'][i]:\n",
    "        sen.append(para[index])\n",
    "    correct_sp.append(sen)\n",
    "validation_data_with_performance['Obs_List'] = correct_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_with_performance.drop(['SE_Index', 'Label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_with_performance.drop(['Sentence_List', 'dev_pred', 'dev_obs'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_mismatch = validation_data_with_performance[validation_data_with_performance['Obs_List'] != validation_data_with_performance['Pred_List']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Length</th>\n",
       "      <th>Pred_List</th>\n",
       "      <th>Obs_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>苏东坡在中国历史上，是哪一个朝代的人？</td>\n",
       "      <td>36</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），]</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），, 北宋时著名的文学家、政治家、艺术家、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>苏东坡是中国哪个省份的人？</td>\n",
       "      <td>36</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），, 眉州眉山（今四川省眉山市）人，]</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），, 眉州眉山（今四川省眉山市）人，, 号...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>苏东坡的爸爸叫什么名字?</td>\n",
       "      <td>36</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），]</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），, 号东坡居士、铁冠道人。, 苏轼的散文...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>苏文忠公指的是谁?</td>\n",
       "      <td>36</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），]</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），, 加赐谥号文忠，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>《苏文忠公全集》是由何人编纂？</td>\n",
       "      <td>36</td>\n",
       "      <td>[苏轼（1037年1月8日－1101年8月24日），, 编有《苏文忠公全集》。]</td>\n",
       "      <td>[宋人王宗稷收其作品，, 编有《苏文忠公全集》。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>内政部消防署提出防范纵火方法「三从四得」，请问是哪「三从」?</td>\n",
       "      <td>33</td>\n",
       "      <td>[可以依照「三从四得」的方法防范纵火，, 民众可以依照「三从四得」的方法，, 防范居家或社区...</td>\n",
       "      <td>[\\n内政部表示，, 民众可以依照「三从四得」的方法，, 三从就是「从消除死角做起」、「从守...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>内政部消防署提出防范纵火方法「三从四得」，请问是哪「四得」?</td>\n",
       "      <td>33</td>\n",
       "      <td>[可以依照「三从四得」的方法防范纵火，, 民众可以依照「三从四得」的方法，, 防范居家或社区...</td>\n",
       "      <td>[\\n内政部表示，, 民众可以依照「三从四得」的方法，, 四得就是「可疑状况要认得」、「大门...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>这起诈骗案件的受害人是男性还是女性？</td>\n",
       "      <td>39</td>\n",
       "      <td>[经查系单身女子曾女数月前上网至「异性」交友网站，, 被骗曾姓女子信以为真，, 遂依指示至南...</td>\n",
       "      <td>[南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 经查系单身...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>这起诈骗案件的诈骗金额是多少？</td>\n",
       "      <td>39</td>\n",
       "      <td>[警方查证后确认系一椿网路交友爱情诈骗案件，]</td>\n",
       "      <td>[南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 当日接获该...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>这起诈骗案件中受害人与诈骗者是否见过面？</td>\n",
       "      <td>39</td>\n",
       "      <td>[南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 经查系单身...</td>\n",
       "      <td>[南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 结识不详外...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Question  Length  \\\n",
       "0               苏东坡在中国历史上，是哪一个朝代的人？      36   \n",
       "1                     苏东坡是中国哪个省份的人？      36   \n",
       "2                      苏东坡的爸爸叫什么名字?      36   \n",
       "3                         苏文忠公指的是谁?      36   \n",
       "4                   《苏文忠公全集》是由何人编纂？      36   \n",
       "..                              ...     ...   \n",
       "237  内政部消防署提出防范纵火方法「三从四得」，请问是哪「三从」?      33   \n",
       "238  内政部消防署提出防范纵火方法「三从四得」，请问是哪「四得」?      33   \n",
       "244              这起诈骗案件的受害人是男性还是女性？      39   \n",
       "245                 这起诈骗案件的诈骗金额是多少？      39   \n",
       "246            这起诈骗案件中受害人与诈骗者是否见过面？      39   \n",
       "\n",
       "                                             Pred_List  \\\n",
       "0                          [苏轼（1037年1月8日－1101年8月24日），]   \n",
       "1         [苏轼（1037年1月8日－1101年8月24日），, 眉州眉山（今四川省眉山市）人，]   \n",
       "2                          [苏轼（1037年1月8日－1101年8月24日），]   \n",
       "3                          [苏轼（1037年1月8日－1101年8月24日），]   \n",
       "4             [苏轼（1037年1月8日－1101年8月24日），, 编有《苏文忠公全集》。]   \n",
       "..                                                 ...   \n",
       "237  [可以依照「三从四得」的方法防范纵火，, 民众可以依照「三从四得」的方法，, 防范居家或社区...   \n",
       "238  [可以依照「三从四得」的方法防范纵火，, 民众可以依照「三从四得」的方法，, 防范居家或社区...   \n",
       "244  [经查系单身女子曾女数月前上网至「异性」交友网站，, 被骗曾姓女子信以为真，, 遂依指示至南...   \n",
       "245                            [警方查证后确认系一椿网路交友爱情诈骗案件，]   \n",
       "246  [南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 经查系单身...   \n",
       "\n",
       "                                              Obs_List  \n",
       "0    [苏轼（1037年1月8日－1101年8月24日），, 北宋时著名的文学家、政治家、艺术家、...  \n",
       "1    [苏轼（1037年1月8日－1101年8月24日），, 眉州眉山（今四川省眉山市）人，, 号...  \n",
       "2    [苏轼（1037年1月8日－1101年8月24日），, 号东坡居士、铁冠道人。, 苏轼的散文...  \n",
       "3                 [苏轼（1037年1月8日－1101年8月24日），, 加赐谥号文忠，]  \n",
       "4                            [宋人王宗稷收其作品，, 编有《苏文忠公全集》。]  \n",
       "..                                                 ...  \n",
       "237  [\\n内政部表示，, 民众可以依照「三从四得」的方法，, 三从就是「从消除死角做起」、「从守...  \n",
       "238  [\\n内政部表示，, 民众可以依照「三从四得」的方法，, 四得就是「可疑状况要认得」、「大门...  \n",
       "244  [南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 经查系单身...  \n",
       "245  [南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 当日接获该...  \n",
       "246  [南港分局南港派出所于108年11月5日接获民众报案称其友人疑似遭受诈骗案件，, 结识不详外...  \n",
       "\n",
       "[198 rows x 4 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Baseline Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGC_LSTM_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        \n",
    "        super(FGC_LSTM_Network, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size * 2, 1)\n",
    "        # (H-state and c_state must be converted into 3d in forward function)\n",
    "    def forward(self, batch):\n",
    "        # batch['ids'] = (batch_size*number of sentence, sent_len)\n",
    "        # batch['segment_ids'] = (batch_size*number of sentence, sent_len)\n",
    "        # batch['mask_ids'] = (batch_size*number of sentence, sent_len)\n",
    "        # pooler_output = (batch_size, 768)\n",
    "        # hidden_state = (batch_size, sent_len, 768)\n",
    "        # output = (batch_size, 1)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers*2, batch['ids'].shape[0], self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers*2, batch['ids'].shape[0], self.hidden_size).to(device)\n",
    "        \n",
    "        batch_size = batch['ids'].shape[0]\n",
    "        max_sent_size = batch['ids'].shape[2]\n",
    "        \n",
    "        ids = batch['ids'].view(-1, max_sent_size)\n",
    "        mask_ids = batch['mask_ids'].view(-1, max_sent_size)\n",
    "        segment_ids = batch['segment_ids'].view(-1, max_sent_size)\n",
    "        \n",
    "        hidden_state, pooler_output = self.bert(ids, mask_ids, segment_ids)\n",
    "        pooler_output = pooler_output.view(batch_size, -1, 768)\n",
    "        \n",
    "        lstm_output, (hn, cn) = self.lstm(pooler_output, (h0, c0))\n",
    "        linear_output = self.linear(lstm_output)\n",
    "        \n",
    "        return linear_output\n",
    "    def loss(self, batch):\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        output = self.forward(batch)\n",
    "        target = batch['labels'].float().to(device)\n",
    "        \n",
    "        return loss_fn(output, target)\n",
    "    \n",
    "    def _predict(self, batch):\n",
    "        \n",
    "        output = self.forward(batch)\n",
    "        scores = torch.sigmoid(output)\n",
    "        scores = scores.cpu().numpy().tolist()\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict_fgc(self, batch, threshold=0.5):\n",
    "        \n",
    "        \n",
    "        scores = self._predict(batch)\n",
    "        max_i = 0\n",
    "        max_score = 0\n",
    "        sp = []\n",
    "    \n",
    "        for i, score in enumerate(scores[0]):\n",
    "\n",
    "            if score[0] > max_score:\n",
    "                max_i = i\n",
    "                max_score = score[0]\n",
    "            if score[0] >= threshold:\n",
    "                sp.append(i)\n",
    "\n",
    "        if not sp:\n",
    "            sp.append(max_i)\n",
    "        # This is to ensure there's no empty supporting evidences\n",
    "        if not sp:\n",
    "            sp.append(max_i)\n",
    "        return {'sp': sp, 'sp_scores': scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0723 08:26:16.695196 140283865257792 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "I0723 08:26:16.701801 140283865257792 configuration_utils.py:169] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0723 08:26:17.582176 140283865257792 modeling_utils.py:387] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n"
     ]
    }
   ],
   "source": [
    "new_network = FGC_LSTM_Network(768, 768, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FGC_LSTM_Network(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(768, 768, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=1536, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Improved Model & Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model_eval(network, dev_batches, current_epoch, sp_golds, avg_loss):\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        sp_preds = []\n",
    "        \n",
    "        for batch in tqdm(dev_batches):\n",
    "            out_dct= network.predict_fgc(batch)\n",
    "            sp_preds.append(out_dct['sp'])\n",
    "                \n",
    "              \n",
    "    metrics = eval_sp_fgc(sp_golds, sp_preds)\n",
    "    print('epoch %d eval_recall: %.3f eval_f1: %.3f loss: %.3f' % (\n",
    "            current_epoch, metrics['sp_recall'], metrics['sp_f1'], avg_loss))\n",
    "        \n",
    "    torch.save(network.state_dict(), \"New_Models/model_epoch{0}_eval_em:{1:.3f}_precision:{2:.3f}_recall:{3:.3f}_f1:{4:.3f}_loss:{5:.3f}.m\".format(current_epoch, metrics['sp_em'], metrics['sp_prec'], metrics['sp_recall'], metrics['sp_f1'], avg_loss))\n",
    "    \n",
    "    return sp_preds, sp_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, data, dev_batches, num_epochs, lr):\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer, scheduler = optim(network, num_epochs, lr)\n",
    "    \n",
    "    sp_golds = validation_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist()\n",
    "    \n",
    "    for current_epoch in range(num_epochs):\n",
    "        network.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(data):\n",
    "            optimizer.zero_grad()\n",
    "            current_output = network(batch)\n",
    "            current_target = batch['labels'].to(dtype=torch.float, device=device)\n",
    "            current_loss = loss_fn(current_output, current_target)\n",
    "\n",
    "            current_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(network.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += current_loss.item()\n",
    "            \n",
    "        learning_rate_scalar = scheduler.get_lr()[0]\n",
    "        print('lr = %f' % learning_rate_scalar)\n",
    "        avg_loss = running_loss/len(data)\n",
    "        print('epoch %d train_loss: %.3f' % (current_epoch, avg_loss))\n",
    "        new_model_eval(network, dev_batches, current_epoch, sp_golds, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev_batches = window_sentence_preprocessing(validation_data, dev_dataset, 10)\n",
    "testing_dev_batches = DataLoader(new_dev_batches, batch_size=2, shuffle = False, collate_fn = collate_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = eval_preprocessing(validation_data, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in a:\n",
    "    data['ids'] = data['ids'].unsqueeze(0)\n",
    "    data['mask_ids'] = data['mask_ids'].unsqueeze(0)\n",
    "    data['segment_ids'] = data['segment_ids'].unsqueeze(0)\n",
    "    data['labels'] = data['labels'].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 101, 5722,  691,  ...,    0,    0,    0],\n",
       "         [ 101, 5722,  691,  ...,    0,    0,    0],\n",
       "         [ 101, 5722,  691,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 5722,  691,  ..., 7674, 8039,  102],\n",
       "         [ 101, 5722,  691,  ...,    0,    0,    0],\n",
       "         [ 101, 5722,  691,  ...,    0,    0,    0]]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model_eval(new_network, a, 0, validation_data['QUESTIONS'].apply(lambda x: x[0]['SHINT_']).tolist(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 48])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(new_dev_batches).next()['ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7c10ed0abe6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train_3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-1a42d25a5f91>\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "sc = new_network._predict(iter(dataloader_train_3d).next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wang: 0.5312566161155701\n",
      "wang: 0.521264910697937\n",
      "wang: 0.5146143436431885\n",
      "wang: 0.5118281841278076\n",
      "wang: 0.5112723112106323\n",
      "wang: 0.5121220946311951\n",
      "wang: 0.5141165256500244\n",
      "wang: 0.5170711278915405\n",
      "wang: 0.5210497379302979\n",
      "wang: 0.5272684693336487\n",
      "wang: 0.47272515296936035\n",
      "wang: 0.4636615812778473\n",
      "wang: 0.4688776731491089\n",
      "wang: 0.48730167746543884\n",
      "wang: 0.4750833213329315\n",
      "wang: 0.4615252912044525\n",
      "wang: 0.46430811285972595\n",
      "wang: 0.4680739641189575\n",
      "wang: 0.48677390813827515\n",
      "wang: 0.49829909205436707\n"
     ]
    }
   ],
   "source": [
    "for batch in sc.cpu().detach().numpy().tolist():\n",
    "    for instance in batch:\n",
    "        print(\"wang:\", instance[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092493740acb44d0bb4545e7117d93af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1807), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr = 0.000005\n",
      "epoch 0 train_loss: 0.479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1208625a3a4df8a668206946f7a375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sp_em': 0.04, 'sp_prec': 0.259, 'sp_recall': 0.132, 'sp_f1': 0.168}\n",
      "epoch 0 eval_recall: 0.132 eval_f1: 0.168 loss: 0.479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf1a48d5bfe483e81c858c97b72e8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1807), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(new_network, dataloader_train_3d, a, 20, 0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': tensor([[[ 101, 5031, 2190,  ...,    0,    0,    0],\n",
      "         [ 101, 5031, 2190,  ...,    0,    0,    0],\n",
      "         [ 101, 5031, 2190,  ..., 2785, 8024,  102],\n",
      "         ...,\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1762,  779,  ...,    0,    0,    0],\n",
      "         [ 101, 1762,  779,  ...,    0,    0,    0],\n",
      "         [ 101, 1762,  779,  ...,    0,    0,    0],\n",
      "         ...,\n",
      "         [ 101, 1762,  779,  ...,    0,    0,    0],\n",
      "         [ 101, 1762,  779,  ...,    0,    0,    0],\n",
      "         [ 101, 1762,  779,  ...,    0,    0,    0]]], device='cuda:0'), 'mask_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0'), 'segment_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0'), 'labels': tensor([[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_train_3d:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
